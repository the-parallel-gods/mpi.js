{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"docs/","title":"Docs","text":""},{"location":"docs/#classes","title":"Classes","text":"Diagnostics <p>Class that handles the diagnostics for the MPI Core. It is used to profile the time taken by different functions in the MPI Core.</p> MPI_Request <p>Class that is returned by the non-blocking functions in the MPI Core. It can be used to test if the request is done or to wait for the request to be done.</p> Packet <p>Class for representing a packet that can be sent between NodeRouter instances.</p> NodeRouter <p>Class for a NodeRouter can be used to send and receive packets between workers on this browser tab.</p> Map2D <p>Class for a map that can be accessed by two keys. All operations are O(1).</p> ProducerConsumer <p>Class for a producer-consumer buffer that can be used to send and receive objects between workers.</p> SmartDashboard <p>Class that handles the SmartDashboard for the MPI Core. SmartDashboard is used to send real-time telemetry data to the main UI process.</p>"},{"location":"docs/#functions","title":"Functions","text":"MPI_Barrier() \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Barrier is a synchronization function that blocks the processes until all processes have reached the barrier. This function is blocking, and it will only return after all the processes have reached the barrier.</p> <p>If optimization flag is set, SSMR (Single Source Multiple Recipients) will be utilized where applicable.</p> MPI_Bcast(data_ptr, root) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Bcast broadcasts data from the root process to all other processes. This function is blocking, and it will only return after all the processes have received the data.</p> <p>If optimization flag is set, SSMR (Single Source Multiple Recipients) will be utilized where applicable.</p> <p>Another optimizatin made is sending parallel messages. If there is a lot of bcasts from the same  gr_id, then the later ones don't need to wait for the earlier ones to propogate every other gr_id.  The root can move on whenever it's done within the local gr. This parallelism maintains correctness  and can offer up to 300x speedup.</p> MPI_Ibcast(data_ptr, root) <p>MPI_Ibcast broadcasts data from the root process to all other processes. This function is non-blocking, and it will return immediately after the data has been broadcasted. The user can use the MPI_Request object to test if the broadcast is done, or to wait for the broadcast to be done.</p> <p>If optimization flag is set, SSMR (Single Source Multiple Recipients) will be utilized where applicable.</p> MPI_Gather(send_ptr, recv_ptr, root) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Gather is a collective operation that gathers data from all processes and stores it in the root process. All processes must provide the same count of data.</p> MPI_Allgather(send_ptr, recv_ptr) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Allgather is a collective operation that gathers data from all processes and stores it in all processes. All processes must provide the same count of data.</p> MPI_Gatherv(send_ptr, recv_ptr, counts, offsets, root) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Gatherv is a collective operation that gathers data from all processes and stores it in the root process. Each process can provide a different count of data. This information must be provided.</p> MPI_Allgatherv(send_ptr, recv_ptr, counts, offsets) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Allgatherv is a collective operation that gathers data from all processes and stores it in all processes. Each process can provide a different count of data. This information must be provided.</p> reschedule(callback) <p>This function is used to take a function off of the event queue  and add it to the back of the event queue.</p> <p>Sometimes, the main function is too long, and it blocks any message events from being processed. This function allows the main function to be paused and other events to be processed. After other queued  events are processed, the main function is added back to the end of event queue to continue processing.</p> flush_telemetry() <p>Sometimes the main function is too long and prevents the telemetry from being flushed. This function manually flushes the telemetry.</p> finish_setup() <p>This function is used to finish setting up this worker after the initial configuration is received.</p> on_init_message(event) <p>This function is used to receive a message from the main process in the initialization phase. After initialization is complete, this function is no longer used.</p> main(main_fn, worker_self) <p>The entry point for user code. The user supplies its main function,  as well as the worker object itself.</p> <p>This function sets up all the necessary components for the worker to run.</p> MPI_Comm_rank(rank_ptr) <p>MPI function to obtain the rank of the current process.</p> MPI_Comm_size(size_ptr) <p>MPI function to obtain the number of processes.</p> MPI_Init() <p>MPI function to initialize the MPI environment.</p> MPI_Finalize() <p>MPI function to finalize and finish the MPI environment.</p> box(data) \u21d2 <code>Box</code> <p>Function to wrap data in a box.</p> unbox(box) \u21d2 <code>any</code> <p>Function to unbox data from a box.</p> MPI_Test(request) \u21d2 <code>Promise.&lt;boolean&gt;</code> <p>MPI_Test tests if the request is done. Regardless of whether the request is done or not, the callback should immediately return a boolean.</p> MPI_Wait(request) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Wait waits for the request to be done. The callback should return a promise that resolves when the request is done.</p> MPI_Send(data_ptr, dest_pid, start, count) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Send sends data to another process. This function is blocking, and it will only return after the receiver confirms that it has received the data.</p> <p>start and count are optional parameters that allow the user to specify a slice of the array to send. Only use these parameters if the data is an array.</p> MPI_Isend(data_ptr, dest_pid, start, count) \u21d2 <code>Promise.&lt;MPI_Request&gt;</code> <p>MPI_Isend sends data to another process. This function is non-blocking, and it will return immediately after sending the data. The user can use the MPI_Request object to test if the buffer is ready to be reused.</p> <p>start and count are optional parameters that allow the user to specify a slice of the array to send. Only use these parameters if the data is an array.</p> MPI_Recv(data_ptr, src_pid, start, count) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Recv receives data from another process. This function is blocking, and it will only return after the data has been received.</p> <p>start and count are optional parameters that allow the user to specify a slice of the array to receive. Only use these parameters if the data is an array.</p> MPI_Irecv(data_ptr, src_pid, start, count) \u21d2 <code>Promise.&lt;MPI_Request&gt;</code> <p>MPI_Irecv receives data from another process. This function is non-blocking, and it will return immediately after receiving the data. The user can use the MPI_Request object to test if the receive is done, or to wait for the receive to be done.</p> <p>start and count are optional parameters that allow the user to specify a slice of the array to receive. Only use these parameters if the data is an array.</p> MPI_Reduce(send_ptr, recv_ptr, operation) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Reduce is a collective operation that combines the data from all processes in the communicator and returns the result to a single process. The root process will receive the result.</p> <p>The operation is function performed on the data pointed to by the data_ptr. The operation is defined by the operation parameter.</p> <p>The optimized version does a local reduce and then send the local results to the root process for a final reduce in order to minimize communication bandwidth.</p> <p>TODO: Future idea: If not a crossbar interconnect, try to reduce along the way.</p> MPI_Allreduce_local_optimized(send_ptr, recv_ptr, operation) \u21d2 <code>Promise.&lt;void&gt;</code> <p>This function only performs a all_reduce operation in the local group of processes. It detects the type of interconnect and uses the appropriate strategy to perform the all_reduce operation most efficiently. The speedup is more significant when the array size is large.</p> <p>Specifically, it uses the ring allreduce strategy for crossbar and ring interconnects, and the tree allreduce strategy for the tree interconnect.</p> MPI_Allreduce(send_ptr, recv_ptr, operation) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Allreduce is a collective operation that combines the data from all processes in the communicator and returns the result to all processes. The result is stored in the recv_ptr box.</p> <p>If the optimized flag is set, and the operation is limited to the local group of processes, then the allreduce operation is performed using the optimized strategy depending on the interconnect type. If the operation spans multiple groups of processes, then the a local reduce is performed first, and a secondary reduce is performed between the groups to save bandwidth. This can have many times speedup when the array size or number of processors is large.</p> MPI_Scatter(send_ptr, recv_ptr, root) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Scatter is a collective operation that scatters data from the root process to all other processes. All processes must provide the same count of data.</p> MPI_Scatterv(send_ptr, recv_ptr, root) \u21d2 <code>Promise.&lt;void&gt;</code> <p>MPI_Scatterv is a collective operation that scatters data from the root process to all other processes. All processes must provide the same count of data.</p> partition(count, num_proc) \u21d2 <code>Object</code> <p>This function partitions the work equally to the number of processors.</p> <p>For example, if there is 6 work and 4 processors, the function will return [2, 2, 1, 1] and [0, 2, 4, 5].</p> <p>Note that it will not return [2, 2, 2, 0] and [0, 2, 4, 6] because the work  is not evenly distributed. The result will have difference of at most 1.</p> make_wrap(n) \u21d2 <code>function</code> <p>This function makes a wrap function that wraps the index around the array.</p> create_crossbar(num_nodes) \u21d2 <code>Object</code> <p>This function creates channels between all pairs of workers. This function does not create the channels object, but rather pairs of ints that represent the workers that are connected.</p> <p>This function also provides the routing table for each node,  so they know where to send messages when they need to communicate with different nodes.</p> create_ring(num_nodes) \u21d2 <code>Object</code> <p>This function creates channels between some pairs of workers. This function does not create the channels object, but rather pairs of ints that represent the workers that are connected.</p> <p>The pattern being created is a ring structure.</p> <p>This function also provides the routing table for each node, so they know where to send messages when they need to communicate with different nodes.</p> <p>For example, if we have 4 workers, the following channels will be created:</p> <p>0 &lt;---&gt; 1</p> <p>1 &lt;---&gt; 2</p> <p>2 &lt;---&gt; 3</p> <p>3 &lt;---&gt; 0</p> <p>For N workers, the number of channels created is O(N)</p> <p>The longest path between any two workers is O(N)</p> create_tree_pow_of_2(num_nodes) \u21d2 <code>Object</code> <p>This function creates channels between some pairs of workers. This function does not create the channels object, but rather pairs of ints that represent the workers that are connected.</p> <p>The pattern being created is a tree structure.</p> <p>This function also provides the routing table for each node, so they know where to send messages when they need to communicate with different nodes.</p> <p>Important note: This function only works when the number of workers is a power of 2.</p> <p>For example, if we have 8 workers, the following channels will be created:</p> <p>0 &lt;---&gt; 1</p> <p>2 &lt;---&gt; 3</p> <p>4 &lt;---&gt; 5</p> <p>6 &lt;---&gt; 7</p> <p>0 &lt;---&gt; 2</p> <p>4 &lt;---&gt; 6</p> <p>0 &lt;---&gt; 4</p> <p>For N workers, the number of channels created is O(N)</p> <p>The longest path between any two workers is O(log(N))</p> create_tree(num_nodes) \u21d2 <code>Object</code> <p>This function creates channels between some pairs of workers. This function does not create the channels object, but rather pairs of ints that represent the workers that are connected.</p> <p>The pattern being created is a tree structure.</p> <p>This function also provides the routing table for each node, so they know where to send messages when they need to communicate with different nodes.</p> <p>This function works for any number of workers, even if it is not a power of 2.</p> <p>For example, if we have 7 workers, the following channels will be created:</p> <p>0 &lt;---&gt; 1</p> <p>2 &lt;---&gt; 3</p> <p>4 &lt;---&gt; 5</p> <p>0 &lt;---&gt; 2</p> <p>4 &lt;---&gt; 6</p> <p>0 &lt;---&gt; 4</p> <p>For N workers, the number of channels created is O(N)</p> <p>The longest path between any two workers is O(log(N))</p>"},{"location":"docs/#typedefs","title":"Typedefs","text":"Box : <code>Object</code> <p>A box holds data. This is used to pass data between functions by reference.</p> Config : <code>Object</code> <p>config</p> Key_A : <code>any</code> Key_B : <code>any</code> SmartDashboardType : <code>'pie'</code> | <code>'progress'</code> | <code>'graph'</code> | <code>'string'</code> <p></p>"},{"location":"docs/#diagnostics","title":"Diagnostics","text":"<p>Class that handles the diagnostics for the MPI Core. It is used to profile the time taken by different functions in the MPI Core.</p> <p>Kind: global class  </p> <ul> <li>Diagnostics<ul> <li>new Diagnostics()</li> <li>.add_send</li> <li>.add_recv</li> <li>.profile \u21d2 <code>function</code></li> <li>.flush</li> <li>.configure(smartdashboard, enabled, period)</li> </ul> </li> </ul> <p></p>"},{"location":"docs/#new-diagnostics","title":"new Diagnostics()","text":"<p>Nothing is done here because the configuration is done by the configure function.</p> <p></p>"},{"location":"docs/#diagnosticsadd_send","title":"diagnostics.add_send","text":"<p>Diagnostic function to count the number of sends in node_router.</p> <p>Kind: instance property of <code>Diagnostics</code> </p>"},{"location":"docs/#diagnosticsadd_recv","title":"diagnostics.add_recv","text":"<p>Diagnostic function to count the number of receives in node_router.</p> <p>Kind: instance property of <code>Diagnostics</code> </p>"},{"location":"docs/#diagnosticsprofile-function","title":"diagnostics.profile \u21d2 <code>function</code>","text":"<p>This function is a decorator that profiles the time taken by a function.</p> <p>Kind: instance property of <code>Diagnostics</code> Returns: <code>function</code> - A new function that profiles the time taken by the original function.  </p> Param Type Description fn <code>function</code> The function to profile. <p></p>"},{"location":"docs/#diagnosticsflush","title":"diagnostics.flush","text":"<p>This function is used to flush the diagnostics to the SmartDashboard. It sends the total time used by each function and the delta time used by each function.</p> <p>Kind: instance property of <code>Diagnostics</code> </p>"},{"location":"docs/#diagnosticsconfiguresmartdashboard-enabled-period","title":"diagnostics.configure(smartdashboard, enabled, period)","text":"<p>This function is used to configure the diagnostics. The configuration is done  here after the object is created because many functions use the profile function of this class before the smartdashboard is created.</p> <p>Kind: instance method of <code>Diagnostics</code> </p> Param Type Default Description smartdashboard <code>SmartDashboard</code> The SmartDashboard object to send the diagnostics to. enabled <code>boolean</code> Whether the diagnostics are collected or not. period <code>number</code> <code>250</code> The period at which the diagnostics are sent to the SmartDashboard. <p></p>"},{"location":"docs/#mpi_request","title":"MPI_Request","text":"<p>Class that is returned by the non-blocking functions in the MPI Core. It can be used to test if the request is done or to wait for the request to be done.</p> <p>Kind: global class  </p> <ul> <li>MPI_Request<ul> <li>new MPI_Request(done)</li> <li>.set_test_callback(callback) \u21d2 <code>MPI_Request</code></li> <li>.set_wait_callback(callback) \u21d2 <code>MPI_Request</code></li> <li>.test() \u21d2 <code>Promise.&lt;boolean&gt;</code></li> <li>.wait() \u21d2</li> </ul> </li> </ul> <p></p>"},{"location":"docs/#new-mpi_requestdone","title":"new MPI_Request(done)","text":"Param Type Default Description done <code>boolean</code> <code>false</code> Whether the request is done or not."},{"location":"docs/#mpi_requestset_test_callbackcallback-mpi_request","title":"mpI_Request.set_test_callback(callback) \u21d2 <code>MPI_Request</code>","text":"<p>Set the test callback for the MPI_Request.  This callback is used to test if the request is done. Regardless of whether the request is done or not,  the callback should immediately return a boolean.</p> <p>Kind: instance method of <code>MPI_Request</code> Returns: <code>MPI_Request</code> - The current MPI_Request object.  </p> Param Type Description callback <code>function</code> The callback to set. <p></p>"},{"location":"docs/#mpi_requestset_wait_callbackcallback-mpi_request","title":"mpI_Request.set_wait_callback(callback) \u21d2 <code>MPI_Request</code>","text":"<p>Set the wait callback for the MPI_Request. This callback is used to wait for the request to be done. The callback should return a promise that resolves when the request is done.</p> <p>Kind: instance method of <code>MPI_Request</code> Returns: <code>MPI_Request</code> - The current MPI_Request object.  </p> Param Type Description callback <code>function</code> The callback to set. <p></p>"},{"location":"docs/#mpi_requesttest-promiseboolean","title":"mpI_Request.test() \u21d2 <code>Promise.&lt;boolean&gt;</code>","text":"<p>Test if the request is done. Regardless of whether the request is done or not, the callback should immediately return a boolean.</p> <p>Kind: instance method of <code>MPI_Request</code> Returns: <code>Promise.&lt;boolean&gt;</code> - A promise that resolves when the test is done, indicating whether the request is done. </p>"},{"location":"docs/#mpi_requestwait","title":"mpI_Request.wait() \u21d2","text":"<p>Wait for the request to be done.  The callback should return a promise that resolves when the request is done.</p> <p>Kind: instance method of <code>MPI_Request</code> Returns: A promise that resolves when the request is done. </p>"},{"location":"docs/#packet","title":"Packet","text":"<p>Class for representing a packet that can be sent between NodeRouter instances.</p> <p>Kind: global class </p>"},{"location":"docs/#new-packetsrc_pid-dest_pid_arr-tag-data","title":"new Packet(src_pid, dest_pid_arr, tag, data)","text":"Param Type Description src_pid <code>number</code> The source pid of the packet. dest_pid_arr <code>Array.&lt;number&gt;</code> The destination pids of the packet. tag <code>string</code> The tag of the packet. data <code>any</code> The data of the packet."},{"location":"docs/#noderouter","title":"NodeRouter","text":"<p>Class for a NodeRouter can be used to send and receive packets between workers on this browser tab.</p> <p>Kind: global class  </p> <ul> <li>NodeRouter<ul> <li>new NodeRouter(num_proc, my_pid, local_channels, global_channel)</li> <li>.send \u21d2 <code>Promise.&lt;void&gt;</code></li> <li>.receive \u21d2 <code>Promise.&lt;Packet&gt;</code></li> <li>.receive_if_available \u21d2 <code>Promise.&lt;Packet&gt;</code></li> <li>.peek \u21d2 <code>Promise.&lt;Packet&gt;</code></li> </ul> </li> </ul> <p></p>"},{"location":"docs/#new-noderouternum_proc-my_pid-local_channels-global_channel","title":"new NodeRouter(num_proc, my_pid, local_channels, global_channel)","text":"Param Type Description num_proc <code>number</code> number of workers my_pid <code>number</code> the pid of this worker local_channels <code>Record.&lt;number, MessagePort&gt;</code> channels to peers on the my local node global_channel <code>WorkerGlobalScope</code> channel to main manager on this node"},{"location":"docs/#noderoutersend-promisevoid","title":"nodeRouter.send \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>Send a packet to the destination pids. This is a non-blocking function that  returns immediately after sending the packet. The receiving worker can only receive this packet if it is listening for packets with the same tag or ANY tag.</p> <p>Send to your own pid if you want to send to the global router.</p> <p>Kind: instance property of <code>NodeRouter</code> Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the packet is sent.  </p> Param Type Description dest_pid_arr <code>Array.&lt;number&gt;</code> The destination pids of the packet. tag <code>string</code> The tag of the packet. Default is \"NA\". data <code>any</code> The data of the packet. Default is \"\". <p></p>"},{"location":"docs/#noderouterreceive-promisepacket","title":"nodeRouter.receive \u21d2 <code>Promise.&lt;Packet&gt;</code>","text":"<p>Receive a packet from a specific source pid with a specific tag. This is a blocking function that waits until a packet is received from the specified source pid with the specified tag.</p> <p>Kind: instance property of <code>NodeRouter</code> Returns: <code>Promise.&lt;Packet&gt;</code> - A promise that resolves to the received packet.  </p> Param Type Description src_pid <code>number</code> The source pid of the packet. Default is null, which means any source pid. tag <code>string</code> The tag of the packet. Default is null, which means any tag. <p></p>"},{"location":"docs/#noderouterreceive_if_available-promisepacket","title":"nodeRouter.receive_if_available \u21d2 <code>Promise.&lt;Packet&gt;</code>","text":"<p>Receive a packet from a specific source pid with a specific tag. This is a non-blocking function that returns immediately if a packet is not available from the specified source pid with the specified tag.</p> <p>Kind: instance property of <code>NodeRouter</code> Returns: <code>Promise.&lt;Packet&gt;</code> - A promise that resolves to the received packet if available, otherwise null.  </p> Param Type Description src_pid <code>number</code> The source pid of the packet. Default is null, which means any source pid. tag <code>string</code> The tag of the packet. Default is null, which means any tag. <p></p>"},{"location":"docs/#noderouterpeek-promisepacket","title":"nodeRouter.peek \u21d2 <code>Promise.&lt;Packet&gt;</code>","text":"<p>Peek at the packet from a specific source pid with a specific tag. This is a non-blocking function that returns immediately if a packet is not available from the specified source pid with the specified tag.</p> <p>This function will not remove the packet from the buffer.</p> <p>Kind: instance property of <code>NodeRouter</code> Returns: <code>Promise.&lt;Packet&gt;</code> - A promise that resolves to the received packet if available, otherwise null.  </p> Param Type Description src_pid <code>number</code> The source pid of the packet. Default is null, which means any source pid. tag <code>string</code> The tag of the packet. Default is null, which means any tag. <p></p>"},{"location":"docs/#map2d","title":"Map2D","text":"<p>Class for a map that can be accessed by two keys. All operations are O(1).</p> <p>Kind: global class  </p> <ul> <li>Map2D<ul> <li>new Map2D()</li> <li>.add(a, b, value)</li> <li>.get(a, b) \u21d2 <code>any</code></li> <li>.pop(a, b) \u21d2 <code>any</code></li> </ul> </li> </ul> <p></p>"},{"location":"docs/#new-map2d","title":"new Map2D()","text":"<p>The ab_map sorts values by a, then by b. ab_map: a -&gt; b -&gt; [value_arr]</p> <p>The ba_map sorts values by b, then by a. ba_map: b -&gt; a -&gt; [value_arr]</p> <p>When adding a value, it is added to both maps. When searching for a value with a certain a_key, the ab_map is searched. When searching for a value with a certain b_key, the ba_map is searched. When a value is popped, it is popped from both maps.</p> <p>At the cost of twice as many pointers, this allows every operation to be O(1).</p> <p></p>"},{"location":"docs/#map2dadda-b-value","title":"map2D.add(a, b, value)","text":"<p>Add a value to the map with a and b keys.</p> <p>Complexity: O(1)</p> <p>Kind: instance method of <code>Map2D</code> </p> Param Type Description a <code>Key_A</code> The first key. b <code>Key_B</code> The second key. value <code>any</code> The value to add. <p></p>"},{"location":"docs/#map2dgeta-b-any","title":"map2D.get(a, b) \u21d2 <code>any</code>","text":"<p>Get a value from the map with a and b keys. If any key is null, it is a wildcard. Wildcard keys mean it will return the first value it sees, ignoring the  requirement of that key.</p> <p>Complexity: O(1)</p> <p>Kind: instance method of <code>Map2D</code> Returns: <code>any</code> - The value found. If no value is found, it returns null.  </p> Param Type Default Description a <code>Key_A</code> <code></code> The first key. Default is null, which means wildcard. b <code>Key_B</code> <code></code> The second key. Default is null, which means wildcard. <p></p>"},{"location":"docs/#map2dpopa-b-any","title":"map2D.pop(a, b) \u21d2 <code>any</code>","text":"<p>Pop a value from the map with a and b keys. If any key is null, it is a wildcard. Wildcard keys mean it will return the first value it sees, ignoring the  requirement of that key.</p> <p>If a value is foundk, the value is removed from the data structure.</p> <p>Complexity: O(1)</p> <p>Kind: instance method of <code>Map2D</code> Returns: <code>any</code> - The value found. If no value is found, it returns null.  </p> Param Type Default Description a <code>Key_A</code> <code></code> The first key. Default is null, which means wildcard. b <code>Key_B</code> <code></code> The second key. Default is null, which means wildcard. <p></p>"},{"location":"docs/#producerconsumer","title":"ProducerConsumer","text":"<p>Class for a producer-consumer buffer that can be used to send and receive objects between workers.</p> <p>Kind: global class  </p> <ul> <li>ProducerConsumer<ul> <li>new ProducerConsumer()</li> <li>.produce(object) \u21d2 <code>Promise.&lt;void&gt;</code></li> <li>.consume(src_pid, tag) \u21d2 <code>Promise.&lt;Packet&gt;</code></li> <li>.consume_if_available(src_pid, tag) \u21d2 <code>Promise.&lt;Packet&gt;</code></li> <li>.peek(src_pid, tag) \u21d2 <code>Promise.&lt;Packet&gt;</code></li> </ul> </li> </ul> <p></p>"},{"location":"docs/#new-producerconsumer","title":"new ProducerConsumer()","text":"<p>Uses two Map2D objects to store the buffer and the callbacks. This way, each call to produce and consume is O(1).</p> <p>TLDR:  For each msg that arrives, it O(1) searches for matching callbacks in callbacks Map2D that can match it.  If no one can take it, the msg will be added to the msgs Map2D.</p> <p>For each consumer callback that is received, it O(1) searches for a msg that can match it.  If no such msg is found, the callback will be added to the callbacks Map2D.</p> <p></p>"},{"location":"docs/#producerconsumerproduceobject-promisevoid","title":"producerConsumer.produce(object) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>Add a packet to the buffer. If a callback is waiting for a packet with the a matching pid and tag, the callback will be called with the packet. Otherwise, the packet will be added to the buffer.</p> <p>Kind: instance method of <code>ProducerConsumer</code> Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the packet is added to the buffer.  </p> Param Type Description object <code>Packet</code> The object to add to the buffer. <p></p>"},{"location":"docs/#producerconsumerconsumesrc_pid-tag-promisepacket","title":"producerConsumer.consume(src_pid, tag) \u21d2 <code>Promise.&lt;Packet&gt;</code>","text":"<p>Get a packet from the buffer. If a packet is available with the a matching pid and tag, the packet will be returned. Otherwise, the consumer will wait until a packet is available.</p> <p>Kind: instance method of <code>ProducerConsumer</code> Returns: <code>Promise.&lt;Packet&gt;</code> - A promise that resolves to the received packet.  </p> Param Type Default Description src_pid <code>number</code> <code></code> The source pid of the packet. Default is null, which means any source pid. tag <code>string</code> <code>null</code> The tag of the packet. Default is null, which means any tag. <p></p>"},{"location":"docs/#producerconsumerconsume_if_availablesrc_pid-tag-promisepacket","title":"producerConsumer.consume_if_available(src_pid, tag) \u21d2 <code>Promise.&lt;Packet&gt;</code>","text":"<p>Get a packet from the buffer if available. If a packet is available with the a matching pid and tag, the packet will be returned. Otherwise, null will be returned immediately. This is a non-blocking function.</p> <p>Kind: instance method of <code>ProducerConsumer</code> Returns: <code>Promise.&lt;Packet&gt;</code> - A promise that resolves to the received packet if available, otherwise null.  </p> Param Type Default Description src_pid <code>number</code> <code></code> The source pid of the packet. Default is null, which means any source pid. tag <code>string</code> <code>null</code> The tag of the packet. Default is null, which means any tag. <p></p>"},{"location":"docs/#producerconsumerpeeksrc_pid-tag-promisepacket","title":"producerConsumer.peek(src_pid, tag) \u21d2 <code>Promise.&lt;Packet&gt;</code>","text":"<p>Peek at the packet from the buffer. This is a non-blocking function that returns immediately. This function will not remove the packet from the buffer.</p> <p>Kind: instance method of <code>ProducerConsumer</code> Returns: <code>Promise.&lt;Packet&gt;</code> - A promise that resolves to the received packet if available, otherwise null.  </p> Param Type Default Description src_pid <code>number</code> <code></code> The source pid of the packet. Default is null, which means any source pid. tag <code>string</code> <code>null</code> The tag of the packet. Default is null, which means any tag. <p></p>"},{"location":"docs/#smartdashboard","title":"SmartDashboard","text":"<p>Class that handles the SmartDashboard for the MPI Core. SmartDashboard is used to send real-time telemetry data to the main UI process.</p> <p>Kind: global class  </p> <ul> <li>SmartDashboard<ul> <li>.putPie(name, dict, downsample)</li> <li>.putProgress(name, value, downsample)</li> <li>.putGraph(name, value, downsample)</li> <li>.putString(name, value, downsample)</li> <li>.flush()</li> </ul> </li> </ul> <p></p>"},{"location":"docs/#smartdashboardputpiename-dict-downsample","title":"smartDashboard.putPie(name, dict, downsample)","text":"<p>This function is used to put a pie chart in the SmartDashboard.</p> <p>Downsample means to only keep the most recent data point. This is useful for real-time, but should be turned off for logs.</p> <p>Kind: instance method of <code>SmartDashboard</code> </p> Param Type Default Description name <code>string</code> The name of the pie chart. dict <code>Record.&lt;string, number&gt;</code> The data for the pie chart. downsample <code>boolean</code> <code>true</code> Whether to downsample the data or not. <p></p>"},{"location":"docs/#smartdashboardputprogressname-value-downsample","title":"smartDashboard.putProgress(name, value, downsample)","text":"<p>This function is used to put a progress bar in the SmartDashboard.</p> <p>Downsample means to only keep the most recent data point. This is useful for real-time, but should be turned off for logs.</p> <p>Kind: instance method of <code>SmartDashboard</code> </p> Param Type Default Description name <code>string</code> The name of the progress bar. value <code>number</code> The value of the progress bar. downsample <code>boolean</code> <code>true</code> Whether to downsample the data or not. <p></p>"},{"location":"docs/#smartdashboardputgraphname-value-downsample","title":"smartDashboard.putGraph(name, value, downsample)","text":"<p>This function is used to put a graph in the SmartDashboard.</p> <p>Downsample means to only keep the most recent data point. This is useful for real-time, but should be turned off for logs.</p> <p>Kind: instance method of <code>SmartDashboard</code> </p> Param Type Default Description name <code>string</code> The name of the graph. value <code>number</code> The value of the graph. downsample <code>boolean</code> <code>true</code> Whether to downsample the data or not. <p></p>"},{"location":"docs/#smartdashboardputstringname-value-downsample","title":"smartDashboard.putString(name, value, downsample)","text":"<p>This function is used to put a string in the SmartDashboard.</p> <p>Downsample means to only keep the most recent data point. This is useful for real-time, but should be turned off for logs.</p> <p>Kind: instance method of <code>SmartDashboard</code> </p> Param Type Default Description name <code>string</code> The name of the string. value <code>string</code> The value of the string. downsample <code>boolean</code> <code>true</code> Whether to downsample the data or not. <p></p>"},{"location":"docs/#smartdashboardflush","title":"smartDashboard.flush()","text":"<p>This function is used to flush the SmartDashboard.</p> <p>Every time a variable is updated, SmartDashboard will try to flush the data, but it will only flush if the period has passed.</p> <p>So if variables are not updated frequently, some data may not be flushed. Use this function to force a flush.</p> <p>A force flush does not respect the min period. A normal flush respects the min period, and can be called as frequently as you want.</p> <p>Kind: instance method of <code>SmartDashboard</code> </p>"},{"location":"docs/#mpi_barrier-promisevoid","title":"MPI_Barrier() \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Barrier is a synchronization function that blocks the processes until all processes have reached the barrier. This function is blocking, and it will only return after all the processes have reached the barrier.</p> <p>If optimization flag is set, SSMR (Single Source Multiple Recipients) will be utilized where applicable.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when all the processes have reached the barrier. </p>"},{"location":"docs/#mpi_bcastdata_ptr-root-promisevoid","title":"MPI_Bcast(data_ptr, root) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Bcast broadcasts data from the root process to all other processes. This function is blocking, and it will only return after all the processes have received the data.</p> <p>If optimization flag is set, SSMR (Single Source Multiple Recipients) will be utilized where applicable.</p> <p>Another optimizatin made is sending parallel messages. If there is a lot of bcasts from the same  gr_id, then the later ones don't need to wait for the earlier ones to propogate every other gr_id.  The root can move on whenever it's done within the local gr. This parallelism maintains correctness  and can offer up to 300x speedup.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the data has been broadcasted.  </p> Param Type Description data_ptr <code>Box</code> The data to broadcast. root <code>number</code> The root process ID. <p></p>"},{"location":"docs/#mpi_ibcastdata_ptr-root","title":"MPI_Ibcast(data_ptr, root)","text":"<p>MPI_Ibcast broadcasts data from the root process to all other processes. This function is non-blocking, and it will return immediately after the data has been broadcasted. The user can use the MPI_Request object to test if the broadcast is done, or to wait for the broadcast to be done.</p> <p>If optimization flag is set, SSMR (Single Source Multiple Recipients) will be utilized where applicable.</p> <p>Kind: global function  </p> Param Type Description data_ptr <code>Box</code> The data to broadcast. root <code>number</code> The root process ID that broadcasts the data. <p></p>"},{"location":"docs/#mpi_gathersend_ptr-recv_ptr-root-promisevoid","title":"MPI_Gather(send_ptr, recv_ptr, root) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Gather is a collective operation that gathers data from all processes and stores it in the root process. All processes must provide the same count of data.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the gather is done.  </p> Param Type Description send_ptr <code>Box</code> The data to gather. recv_ptr <code>Box</code> The box to store the gathered data. root <code>number</code> The root process ID that gathers the data. <p></p>"},{"location":"docs/#mpi_allgathersend_ptr-recv_ptr-promisevoid","title":"MPI_Allgather(send_ptr, recv_ptr) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Allgather is a collective operation that gathers data from all processes and stores it in all processes. All processes must provide the same count of data.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the gather is done.  </p> Param Type Description send_ptr <code>Box</code> The data to gather. recv_ptr <code>Box</code> The box to store the gathered data. <p></p>"},{"location":"docs/#mpi_gathervsend_ptr-recv_ptr-counts-offsets-root-promisevoid","title":"MPI_Gatherv(send_ptr, recv_ptr, counts, offsets, root) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Gatherv is a collective operation that gathers data from all processes and stores it in the root process. Each process can provide a different count of data. This information must be provided.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the gather is done.  </p> Param Type Description send_ptr <code>Box</code> The data to gather. recv_ptr <code>Box</code> The box to store the gathered data. counts <code>Array.&lt;number&gt;</code> The count of data to gather from each process. offsets <code>Array.&lt;number&gt;</code> The offset to store the data in the recv_ptr. root <code>number</code> The root process ID that gathers the data. <p></p>"},{"location":"docs/#mpi_allgathervsend_ptr-recv_ptr-counts-offsets-promisevoid","title":"MPI_Allgatherv(send_ptr, recv_ptr, counts, offsets) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Allgatherv is a collective operation that gathers data from all processes and stores it in all processes. Each process can provide a different count of data. This information must be provided.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the gather is done.  </p> Param Type Description send_ptr <code>Box</code> The data to gather. recv_ptr <code>Box</code> The box to store the gathered data. counts <code>Array.&lt;number&gt;</code> The count of data to gather from each process. offsets <code>Array.&lt;number&gt;</code> The offset to store the data in the recv_ptr. <p></p>"},{"location":"docs/#reschedulecallback","title":"reschedule(callback)","text":"<p>This function is used to take a function off of the event queue  and add it to the back of the event queue.</p> <p>Sometimes, the main function is too long, and it blocks any message events from being processed. This function allows the main function to be paused and other events to be processed. After other queued  events are processed, the main function is added back to the end of event queue to continue processing.</p> <p>Kind: global function  </p> Param Type Description callback <code>function</code> The function to pause and reschedule. <p></p>"},{"location":"docs/#flush_telemetry","title":"flush_telemetry()","text":"<p>Sometimes the main function is too long and prevents the telemetry from being flushed. This function manually flushes the telemetry.</p> <p>Kind: global function </p>"},{"location":"docs/#finish_setup","title":"finish_setup()","text":"<p>This function is used to finish setting up this worker after the initial configuration is received.</p> <p>Kind: global function </p>"},{"location":"docs/#on_init_messageevent","title":"on_init_message(event)","text":"<p>This function is used to receive a message from the main process in the initialization phase. After initialization is complete, this function is no longer used.</p> <p>Kind: global function  </p> Param Type Description event <code>MessageEvent</code> The message event from the main process. <p></p>"},{"location":"docs/#mainmain_fn-worker_self","title":"main(main_fn, worker_self)","text":"<p>The entry point for user code. The user supplies its main function,  as well as the worker object itself.</p> <p>This function sets up all the necessary components for the worker to run.</p> <p>Kind: global function  </p> Param Type Description main_fn <code>function</code> The main function of the user. worker_self <code>WorkerGlobalScope</code> The worker object itself. <p></p>"},{"location":"docs/#mpi_comm_rankrank_ptr","title":"MPI_Comm_rank(rank_ptr)","text":"<p>MPI function to obtain the rank of the current process.</p> <p>Kind: global function  </p> Param Type Description rank_ptr <code>Box</code> A box to store the rank of the current process. <p></p>"},{"location":"docs/#mpi_comm_sizesize_ptr","title":"MPI_Comm_size(size_ptr)","text":"<p>MPI function to obtain the number of processes.</p> <p>Kind: global function  </p> Param Type Description size_ptr <code>Box</code> A box to store the number of processes. <p></p>"},{"location":"docs/#mpi_init","title":"MPI_Init()","text":"<p>MPI function to initialize the MPI environment.</p> <p>Kind: global function </p>"},{"location":"docs/#mpi_finalize","title":"MPI_Finalize()","text":"<p>MPI function to finalize and finish the MPI environment.</p> <p>Kind: global function </p>"},{"location":"docs/#boxdata-box","title":"box(data) \u21d2 <code>Box</code>","text":"<p>Function to wrap data in a box.</p> <p>Kind: global function Returns: <code>Box</code> - A box containing the data.  </p> Param Type Description data <code>any</code> The data to wrap in a box. <p></p>"},{"location":"docs/#unboxbox-any","title":"unbox(box) \u21d2 <code>any</code>","text":"<p>Function to unbox data from a box.</p> <p>Kind: global function Returns: <code>any</code> - The data from the box.  </p> Param Type Description box <code>Box</code> The box containing the data. <p></p>"},{"location":"docs/#mpi_testrequest-promiseboolean","title":"MPI_Test(request) \u21d2 <code>Promise.&lt;boolean&gt;</code>","text":"<p>MPI_Test tests if the request is done. Regardless of whether the request is done or not, the callback should immediately return a boolean.</p> <p>Kind: global function Returns: <code>Promise.&lt;boolean&gt;</code> - A promise that resolves when the test is done, indicating whether the request is done.  </p> Param Type Description request <code>MPI_Request</code> The request to test. <p></p>"},{"location":"docs/#mpi_waitrequest-promisevoid","title":"MPI_Wait(request) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Wait waits for the request to be done. The callback should return a promise that resolves when the request is done.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the request is done.  </p> Param Type Description request <code>MPI_Request</code> The request to wait for. <p></p>"},{"location":"docs/#mpi_senddata_ptr-dest_pid-start-count-promisevoid","title":"MPI_Send(data_ptr, dest_pid, start, count) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Send sends data to another process. This function is blocking, and it will only return after the receiver confirms that it has received the data.</p> <p>start and count are optional parameters that allow the user to specify a slice of the array to send. Only use these parameters if the data is an array.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the data has been sent.  </p> Param Type Description data_ptr <code>Box</code> The data to send. dest_pid <code>number</code> The destination process ID. start <code>number</code> The start index of the data to send. Default is 0 if only count is specified. count <code>number</code> The number of elements to send. Only use if data is an array. <p></p>"},{"location":"docs/#mpi_isenddata_ptr-dest_pid-start-count-promisempi_request","title":"MPI_Isend(data_ptr, dest_pid, start, count) \u21d2 <code>Promise.&lt;MPI_Request&gt;</code>","text":"<p>MPI_Isend sends data to another process. This function is non-blocking, and it will return immediately after sending the data. The user can use the MPI_Request object to test if the buffer is ready to be reused.</p> <p>start and count are optional parameters that allow the user to specify a slice of the array to send. Only use these parameters if the data is an array.</p> <p>Kind: global function Returns: <code>Promise.&lt;MPI_Request&gt;</code> - A promise that indicates that the data has been sent.  </p> Param Type Description data_ptr <code>Box</code> The data to send. dest_pid <code>number</code> The destination process ID. start <code>number</code> The start index of the data to send. Default is 0 if only count is specified. count <code>number</code> The number of elements to send. Only use if data is an array. <p></p>"},{"location":"docs/#mpi_recvdata_ptr-src_pid-start-count-promisevoid","title":"MPI_Recv(data_ptr, src_pid, start, count) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Recv receives data from another process. This function is blocking, and it will only return after the data has been received.</p> <p>start and count are optional parameters that allow the user to specify a slice of the array to receive. Only use these parameters if the data is an array.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the data has been received.  </p> Param Type Description data_ptr <code>Box</code> The box to store the received data. src_pid <code>number</code> The source process ID. Default is null to receive from any process. start <code>number</code> The start index of the data to receive. Default is 0 if only count is specified. count <code>number</code> The number of elements to receive. Only use if data is an array. <p></p>"},{"location":"docs/#mpi_irecvdata_ptr-src_pid-start-count-promisempi_request","title":"MPI_Irecv(data_ptr, src_pid, start, count) \u21d2 <code>Promise.&lt;MPI_Request&gt;</code>","text":"<p>MPI_Irecv receives data from another process. This function is non-blocking, and it will return immediately after receiving the data. The user can use the MPI_Request object to test if the receive is done, or to wait for the receive to be done.</p> <p>start and count are optional parameters that allow the user to specify a slice of the array to receive. Only use these parameters if the data is an array.</p> <p>Kind: global function Returns: <code>Promise.&lt;MPI_Request&gt;</code> - A promise that indicates that the data has been received.  </p> Param Type Description data_ptr <code>Box</code> The box to store the received data. src_pid <code>number</code> The source process ID. Default is null to receive from any process. start <code>number</code> The start index of the data to receive. Default is 0 if only count is specified. count <code>number</code> The number of elements to receive. Only use if data is an array. <p></p>"},{"location":"docs/#mpi_reducesend_ptr-recv_ptr-operation-promisevoid","title":"MPI_Reduce(send_ptr, recv_ptr, operation) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Reduce is a collective operation that combines the data from all processes in the communicator and returns the result to a single process. The root process will receive the result.</p> <p>The operation is function performed on the data pointed to by the data_ptr. The operation is defined by the operation parameter.</p> <p>The optimized version does a local reduce and then send the local results to the root process for a final reduce in order to minimize communication bandwidth.</p> <p>TODO: Future idea: If not a crossbar interconnect, try to reduce along the way.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when reduce is done.  </p> Param Type Description send_ptr <code>Box</code> The boxed data array to reduce. recv_ptr <code>Box</code> The box to store the result array. operation <code>function</code> The operation to perform on the data. <p></p>"},{"location":"docs/#mpi_allreduce_local_optimizedsend_ptr-recv_ptr-operation-promisevoid","title":"MPI_Allreduce_local_optimized(send_ptr, recv_ptr, operation) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>This function only performs a all_reduce operation in the local group of processes. It detects the type of interconnect and uses the appropriate strategy to perform the all_reduce operation most efficiently. The speedup is more significant when the array size is large.</p> <p>Specifically, it uses the ring allreduce strategy for crossbar and ring interconnects, and the tree allreduce strategy for the tree interconnect.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when reduce is done.  </p> Param Type Description send_ptr <code>Box</code> The boxed data array to reduce. recv_ptr <code>Box</code> The box to store the result array. operation <code>function</code> The operation to perform on the data. <p></p>"},{"location":"docs/#mpi_allreducesend_ptr-recv_ptr-operation-promisevoid","title":"MPI_Allreduce(send_ptr, recv_ptr, operation) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Allreduce is a collective operation that combines the data from all processes in the communicator and returns the result to all processes. The result is stored in the recv_ptr box.</p> <p>If the optimized flag is set, and the operation is limited to the local group of processes, then the allreduce operation is performed using the optimized strategy depending on the interconnect type. If the operation spans multiple groups of processes, then the a local reduce is performed first, and a secondary reduce is performed between the groups to save bandwidth. This can have many times speedup when the array size or number of processors is large.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when reduce is done.  </p> Param Type Description send_ptr <code>Box</code> The boxed data array to reduce. recv_ptr <code>Box</code> The box to store the result array. operation <code>function</code> The operation to perform on the data. <p></p>"},{"location":"docs/#mpi_scattersend_ptr-recv_ptr-root-promisevoid","title":"MPI_Scatter(send_ptr, recv_ptr, root) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Scatter is a collective operation that scatters data from the root process to all other processes. All processes must provide the same count of data.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the scatter is done.  </p> Param Type Description send_ptr <code>Box</code> The data to scatter. recv_ptr <code>Box</code> The box to store the scattered data. root <code>number</code> The root process ID that gathers the data. <p></p>"},{"location":"docs/#mpi_scattervsend_ptr-recv_ptr-root-promisevoid","title":"MPI_Scatterv(send_ptr, recv_ptr, root) \u21d2 <code>Promise.&lt;void&gt;</code>","text":"<p>MPI_Scatterv is a collective operation that scatters data from the root process to all other processes. All processes must provide the same count of data.</p> <p>Kind: global function Returns: <code>Promise.&lt;void&gt;</code> - A promise that resolves when the scatter is done.  </p> Param Type Description send_ptr <code>Box</code> The data to scatter. recv_ptr <code>Box</code> The box to store the scattered data. root <code>number</code> The root process ID that gathers the data. <p></p>"},{"location":"docs/#partitioncount-num_proc-object","title":"partition(count, num_proc) \u21d2 <code>Object</code>","text":"<p>This function partitions the work equally to the number of processors.</p> <p>For example, if there is 6 work and 4 processors, the function will return [2, 2, 1, 1] and [0, 2, 4, 5].</p> <p>Note that it will not return [2, 2, 2, 0] and [0, 2, 4, 6] because the work  is not evenly distributed. The result will have difference of at most 1.</p> <p>Kind: global function Returns: <code>Object</code> - The sizes and start indexes of the work.  </p> Param Type Description count <code>number</code> The total number of work. num_proc <code>number</code> The number of processors. <p></p>"},{"location":"docs/#make_wrapn-function","title":"make_wrap(n) \u21d2 <code>function</code>","text":"<p>This function makes a wrap function that wraps the index around the array.</p> <p>Kind: global function Returns: <code>function</code> - The wrap function that wraps the index around the array.  </p> Param Type Description n <code>number</code> The size of the array. <p></p>"},{"location":"docs/#create_crossbarnum_nodes-object","title":"create_crossbar(num_nodes) \u21d2 <code>Object</code>","text":"<p>This function creates channels between all pairs of workers. This function does not create the channels object, but rather pairs of ints that represent the workers that are connected.</p> <p>This function also provides the routing table for each node,  so they know where to send messages when they need to communicate with different nodes.</p> <p>Kind: global function Returns: <code>Object</code> - The result routing computation  </p> Param Type Description num_nodes <code>number</code> number of workers locally that needs to connect <p></p>"},{"location":"docs/#create_ringnum_nodes-object","title":"create_ring(num_nodes) \u21d2 <code>Object</code>","text":"<p>This function creates channels between some pairs of workers. This function does not create the channels object, but rather pairs of ints that represent the workers that are connected.</p> <p>The pattern being created is a ring structure.</p> <p>This function also provides the routing table for each node, so they know where to send messages when they need to communicate with different nodes.</p> <p>For example, if we have 4 workers, the following channels will be created:</p> <p>0 &lt;---&gt; 1</p> <p>1 &lt;---&gt; 2</p> <p>2 &lt;---&gt; 3</p> <p>3 &lt;---&gt; 0</p> <p>For N workers, the number of channels created is O(N)</p> <p>The longest path between any two workers is O(N)</p> <p>Kind: global function Returns: <code>Object</code> - The result routing computation  </p> Param Type num_nodes <code>number</code> <p></p>"},{"location":"docs/#create_tree_pow_of_2num_nodes-object","title":"create_tree_pow_of_2(num_nodes) \u21d2 <code>Object</code>","text":"<p>This function creates channels between some pairs of workers. This function does not create the channels object, but rather pairs of ints that represent the workers that are connected.</p> <p>The pattern being created is a tree structure.</p> <p>This function also provides the routing table for each node, so they know where to send messages when they need to communicate with different nodes.</p> <p>Important note: This function only works when the number of workers is a power of 2.</p> <p>For example, if we have 8 workers, the following channels will be created:</p> <p>0 &lt;---&gt; 1</p> <p>2 &lt;---&gt; 3</p> <p>4 &lt;---&gt; 5</p> <p>6 &lt;---&gt; 7</p> <p>0 &lt;---&gt; 2</p> <p>4 &lt;---&gt; 6</p> <p>0 &lt;---&gt; 4</p> <p>For N workers, the number of channels created is O(N)</p> <p>The longest path between any two workers is O(log(N))</p> <p>Kind: global function Returns: <code>Object</code> - The result routing computation  </p> Param Type Description num_nodes <code>number</code> number of workers locally that needs to connect <p></p>"},{"location":"docs/#create_treenum_nodes-object","title":"create_tree(num_nodes) \u21d2 <code>Object</code>","text":"<p>This function creates channels between some pairs of workers. This function does not create the channels object, but rather pairs of ints that represent the workers that are connected.</p> <p>The pattern being created is a tree structure.</p> <p>This function also provides the routing table for each node, so they know where to send messages when they need to communicate with different nodes.</p> <p>This function works for any number of workers, even if it is not a power of 2.</p> <p>For example, if we have 7 workers, the following channels will be created:</p> <p>0 &lt;---&gt; 1</p> <p>2 &lt;---&gt; 3</p> <p>4 &lt;---&gt; 5</p> <p>0 &lt;---&gt; 2</p> <p>4 &lt;---&gt; 6</p> <p>0 &lt;---&gt; 4</p> <p>For N workers, the number of channels created is O(N)</p> <p>The longest path between any two workers is O(log(N))</p> <p>Kind: global function Returns: <code>Object</code> - The result routing computation  </p> Param Type Description num_nodes <code>number</code> number of workers locally that needs to connect <p></p>"},{"location":"docs/#box-object","title":"Box : <code>Object</code>","text":"<p>A box holds data. This is used to pass data between functions by reference.</p> <p>Kind: global typedef </p>"},{"location":"docs/#config-object","title":"Config : <code>Object</code>","text":"<p>config</p> <p>Kind: global typedef </p>"},{"location":"docs/#key_a-any","title":"Key_A : <code>any</code>","text":"<p>Kind: global typedef </p>"},{"location":"docs/#key_b-any","title":"Key_B : <code>any</code>","text":"<p>Kind: global typedef </p>"},{"location":"docs/#smartdashboardtype-pie-progress-graph-string","title":"SmartDashboardType : <code>'pie'</code> | <code>'progress'</code> | <code>'graph'</code> | <code>'string'</code>","text":"<p>Kind: global typedef  </p>"},{"location":"final/","title":"Final Report","text":""},{"location":"final/#project-mpijs","title":"PROJECT: MPI.JS","text":"<p>MPI, now on the web.</p>"},{"location":"final/#url","title":"URL","text":"<p>https://the-parallel-gods.github.io/mpi.js/home/</p>"},{"location":"final/#the-vision","title":"THE VISION","text":"<p>Our mission is to revolutionize impractical distributed computing by providing a browser-based MPI implementation that empowers researchers and developers to seamlessly explore parallel programming concepts and deploy distributed applications across diverse platforms.</p>"},{"location":"final/#summary","title":"SUMMARY","text":"<p>We want to create an MPI library in the browser using JavaScript, implement some of the APIs (bcast, barrier, all_reduce, ...), use them to run MPI programs, and optimize the MPI collective APIs given the browser environment.</p> <p>We have created a 40-page long documentation website for MPI developers here: </p> <p>https://the-parallel-gods.github.io/mpi.js/docs/</p> <p></p>"},{"location":"final/#table-of-contents","title":"Table of Contents","text":"<ul> <li>PROJECT: MPI.JS</li> <li>URL</li> <li>THE VISION</li> <li>SUMMARY</li> <li>Table of Contents</li> <li>BACKGROUND</li> <li>Processes inside a browser are very isolated</li> <li>JavaScript is event-driven single-threaded language</li> <li>The browser cannot create a WebSocket server</li> <li>Browser comes with a UI</li> <li>SYSTEM ARCHITECTURE</li> <li>Address naming</li> <li>WebSocket Server</li> <li>Static File Server</li> <li>Global Router</li> <li>Node Router</li> <li>Doubly Indexed Database</li> <li>Real-time Dashboard</li> <li>OPTIMIZATION</li> <li>Single Source Multiple Receive (SSMR)</li> <li>Local Allreduce Optimization</li> <li>Global Optimization</li> <li>Latency Hiding</li> <li>RESULTS</li> <li>Local Tests<ul> <li>Local Allreduce</li> <li>Local Barrier</li> <li>Local Bcast</li> </ul> </li> <li>Global Tests<ul> <li>Global Broadcast</li> </ul> </li> <li>Global Barrier<ul> <li>Global Reduce</li> </ul> </li> <li>Conclusion</li> <li>Contribution</li> <li>Sean (haoxians) - 50%</li> <li>David (drudo) - 50%</li> </ul>"},{"location":"final/#background","title":"BACKGROUND","text":"<p>MPI has never been implemented in JavaScript in the browser before, so we are bound to run into many new interesting problems specific to the runtime environment. In this section, we provide some background information on how JavaScript works and describe the challenges it poses. In the next section, we will describe our approach to solving these challenges.</p>"},{"location":"final/#processes-inside-a-browser-are-very-isolated","title":"Processes inside a browser are very isolated","text":"<p>For good security reasons, each process in the browser is very isolated from the others. This means that there is no shared memory between processes, and communication between processes is limited. The only types of communication that are allowed are point-to-point <code>MessageChannel</code>s and <code>BroadcastChannel</code>s. To make things worse, <code>BroadcastChannel</code>s are rate-limited one message per 100ms, which is too slow. Effectively, we are forced to use <code>MessageChannel</code>s for our communication inside a browser.</p>"},{"location":"final/#javascript-is-event-driven-single-threaded-language","title":"JavaScript is event-driven single-threaded language","text":"<p>Normally, JavaScript is designed for UI. What event-driven means in that context is every flow of control is initiated by an event, like a button press. Moreover, since it is single-threaded, no other code gets a chance to run until the function for that event finishes. This is a problem for parallel programming, because we can't just spawn a new thread to do some work in the background. This is a huge limitation when it comes to supporting non-blocking MPI operations.</p>"},{"location":"final/#the-browser-cannot-create-a-websocket-server","title":"The browser cannot create a WebSocket server","text":"<p>In this project, we want to support MPI programs that run across multiple computers over the Internet. Since the browser cannot create raw TCP or UDP sockets, the best option is to use WebSockets, which is a higher-level protocol built on top of TCP. However, since the browser does not have the permission to create servers, we need to have a centralized WebSocket server that all the browsers connect to. This server will be responsible for routing messages between the browsers.</p>"},{"location":"final/#browser-comes-with-a-ui","title":"Browser comes with a UI","text":"<p>Since the browser already provides an HTML UI, we can use it to show the status of the MPI program. In this project, we will take advantage of this by creating a live dashboard that shows the diagnostic information of the MPI program in real-time.</p>"},{"location":"final/#system-architecture","title":"SYSTEM ARCHITECTURE","text":"<p>Our system architecture is inspired by network gateways at the global level and hardware architectures at the local level.  We design our system to have a centralized WebSocket server that all the browsers connect to as well as a static file server. Inside each browser tab, the system creates many worker processes that run the user MPI code, which are all connected by hot-swappable interconnect architectures.</p>"},{"location":"final/#address-naming","title":"Address naming","text":"<p>Since this project involves significant routing work, here we formally clarify the address naming scheme we use. </p> <ul> <li>GR_ID: Global Router ID (unique identifier for each browser)</li> <li>NR_ID: Node Router ID (local unique identifier for each worker process, starts from 0 for each browser)</li> <li>PID: Universal Node Router ID (what user sees; global unique identifier for each worker process, continuous across browsers)</li> <li>NR_OFFSET: Node Router Offset (smallest PID in the local worker pool)</li> </ul> <p>The system is designed this way so that the user can use an abstraction that gives the illusion of every worker process being in the same global network; however, the system under the hood is designed to be as optimized as possible.</p> <p>Example system:</p> <pre><code>Browser: GR_ID=0\n    Worker: PID=0, NR_ID=0, NR_OFFSET=0\n    Worker: PID=1, NR_ID=1, NR_OFFSET=0\n    Worker: PID=2, NR_ID=2, NR_OFFSET=0\n\nBrowser: GR_ID=1\n    Worker: PID=3, NR_ID=0, NR_OFFSET=3\n    Worker: PID=4, NR_ID=1, NR_OFFSET=3\n    Worker: PID=5, NR_ID=2, NR_OFFSET=3\n</code></pre>"},{"location":"final/#websocket-server","title":"WebSocket Server","text":"<p>The WebSocket server is responsible for routing messages between the browser. It assigns each browser a unique GR_ID and keeps track of the global routing table. Since this central server is a point of contention, we designed it to be as lightweight as possible. We also designed our routing protocols to offload as much work as possible to the browser. Each request to the server is a simple JSON object that contains the message and the destination GR_ID. The websocket server uses the SSMR optimization (described later).</p>"},{"location":"final/#static-file-server","title":"Static File Server","text":"<p>The static file server is responsible for serving the user's desired MPI code, the MPI.js library, and the UI files. It is a simple HTTP server that supports hot-loading the user's code into the browser.</p>"},{"location":"final/#global-router","title":"Global Router","text":"<p>Sitting in the Browser's UI process, the Global Router is responsible for routing messages between browsers. Whenever a Node Router wants to send a message to another browser, it delegates the message to the Global Router. The Global Router then forwards the message to the destination browser's Global Router, which then forwards the message to the destination Node Router. When messages get to the Global Router level, the PID and the NR_IDs are abstracted away, and the system only deals with GR_IDs. This is done to make the system more scalable and to hide the complexity of the system at each layer. The Global Router uses SSMR optimization (described later), so if it needs to send the same message to multiple other Global Routers, it only needs to send one message to the WebSocket server.</p>"},{"location":"final/#node-router","title":"Node Router","text":"<p>The Node Router is located in the worker process and is responsible for routing, queueing, and feeding messages to and from the user's MPI code. The Node Router is also handles routing messages between workers within the same browser. The Node Router achieves this with a custom routing table to determine the best route to send a message to another worker process. The Node Router uses SSMR optimization (described later) to reduce the number of messages sent.</p> <p>In the best case, the interconnect that connects the Node Routers within the same browser is a crossbar, which allows any message to be sent to any other worker process within one hop. However, when more workers are needed, the number of connections grows quadratically, so we also support a ring and tree interconnect that balance the number of connections and hops.</p> <p>When ring or tree interconnects are used, the Node Router also serves as a forwarder for messages that need to be sent to another worker. If a node isn't directly connected to the destination node, it will send it to someone closer to the destination, who will then forward it to the destination.</p>"},{"location":"final/#doubly-indexed-database","title":"Doubly Indexed Database","text":"<p>Whenever a Node Router receives a message, it needs to feed that message to the user's MPI code. In our JavaScript MPI implementation, we skip the back and forth checking that actual MPI implementations do in order to improve performance. Instead, we directly deposit the message into a queue. Since the system is in a browser, where memory usage is already very high without MPI, we delegate the responsibility of not overflowing the queue to the MPI user. Since JavaScript is single-threaded and thread-safe, we can construct a very performant ProducerConsumer queue without locks.</p> <p>The logic is as follows: when we receive a message with a tag and src_pid, we check the receiverDB if there is a user waiting for that message. If there is, we directly call the user's callback. If there isn't, we deposit the message into the messageDB. When the user calls a receive function, we check the messageDB for that message. If it is there, we directly call the user's callback. If it isn't, we deposit the user's callback into the receiverDB.</p> <p>Our focus then shifts to making the queue as efficient as possible, since many messages can be waiting there. This would have been simple, if not for the tags and src_pids of the messages. MPI supports having users receive messages with only specific tags and from specific processes. If the number of messages in the queue is very high, the search through all of them to find the right message will be very slow. Our solution is to use a doubly indexed database. When a object is deposited into the queue, it is saved in memory, and we insert the object's tag and src_pid into two separate indices that point to the object. This way, no matter if the user requests according to tag or src_pid, all operations are <code>O(1)</code>.</p>"},{"location":"final/#real-time-dashboard","title":"Real-time Dashboard","text":"<p>One of the key features of our system is the real-time dashboard. The dashboard shows the number of messages sent and received by each worker process, and the proportion of time spent on each MPI operation. The dashboard is updated in real-time, so the user can see how their MPI program is performing, and interpret the animations to debug or optimize their code.</p> <p></p>"},{"location":"final/#optimization","title":"OPTIMIZATION","text":""},{"location":"final/#single-source-multiple-receive-ssmr","title":"Single Source Multiple Receive (SSMR)","text":"<p>One of the key optimizations we implemented was Single Source Multiple Receive. We observed that during a broadcast operation, the same message is sent to multiple workers. But since some messages are forwarded multiple times, the same message is sent multiple times. This is a waste of bandwidth.</p> <p>Thus, we propose to change the destination field of the message to hold multiple destinations. This way, when a message is sent, it is sent to multiple destinations at once. Each forwarder along the way will first check if the message is meant for them, and if it is, they will first consume the message before forwarding it. Finally, the forwarding router will check if the message needs to be sent along multiple paths to reach all the intended destinations. If it does, it will group the recipients to achieve the minimum number of duplications as depicted above.</p> <p>This optimization is implemented at both the local level between the Node Routers and the global level between the Global Routers. This optimization is particularly useful when the local interconnect is a ring or a tree. It is also effective at the global level where the resources are more scarce.</p>"},{"location":"final/#local-allreduce-optimization","title":"Local Allreduce Optimization","text":"<p>At the local level, we implemented an optimized allreduce operation using ring reduce. As described in lecture, the strategy uses significantly less bandwidth than the naive allreduce implementation. Particularly, when the interconnect is a ring, the allreduce operation fits perfectly into the ring structure. </p> <p>For the tree interconnect, we implemented a optimized version as well. In this case, each layer of the tree reduces at the same time, and the root node broadcasts the result to all the other nodes. This is a significant improvement over the naive allreduce implementation, which would have to send the message to every other node.</p>"},{"location":"final/#global-optimization","title":"Global Optimization","text":"<p>As described before, sending messages over the WebSocket server has high latency and low bandwidth. To reduce the number of messages sent, we implemented a divide-and-conquer strategy for MPI_Reduce and MPI_Barrier. For reduce, for example, first, each machine does its local fast reduce to produce a partial result. Then, the operation is raised to the global level, where each machine sends its partial result to perform a secondary reduce. This way, the number of messages sent over the WebSocket server is decreased significantly.</p>"},{"location":"final/#latency-hiding","title":"Latency Hiding","text":"<p>Finally, we implemented a latency hiding strategy for the bcast operation. We discovered that the WebSocket is many orders of magnitude slower than the local communication. Thus, we implemented a strategy where the local workers can keep working if permitted by correctness. This way, the latency of sending messages over the WebSocket server significantly overlaps. We found that the bandwidth isn't the primary bottleneck, so this strategy is particularly effective.</p> <p>In this example, node 1 first sends a bcast to everyone, then node 2 sends a bcast to everyone. However, node 1's message reaches node 2 before node 1's bcast reaches everyone on the other machine. Instead of waiting for the message to propagate fully, node 2 can continue working and send its bcast without violating correctness. This way, the latency of the bcast operation is significantly reduced.</p>"},{"location":"final/#results","title":"RESULTS","text":""},{"location":"final/#local-tests","title":"Local Tests","text":"<p>Due to limited space, we only show the speedup graphs here. For the full results, please see https://github.com/the-parallel-gods/mpi.js/tree/main/docs/images/benchmarks.</p> <p>In the following tests, we run the MPI operations on a single browser with multiple workers. The speedup is calculated by comparing the time taken for the unoptimized version and the optimized version. The optimized version has the SSMR and the local allreduce optimization, while the unoptimized version does not.</p>"},{"location":"final/#local-allreduce","title":"Local Allreduce","text":"<p>When using the crossbar interconnect, communication is relatively fast. Because of this, naively sending the message to all other nodes is slightly faster than doing the ring reduce when the number of nodes is small. As a result, the extra overhead of calculating indices in ring reduce is actually 3x slower. However, as the number of nodes increases, the ring reduce becomes steadily faster than the naive reduce. The fastest speedup is 2x with 32 nodes.</p> <p>When using the ring interconnect, the ring reduce is also slower than the naive reduce when the number of nodes is small. Again, this signifies the significant overhead of ring reduce. However, as the number of nodes increases, the ring reduce becomes significantly faster than the naive reduce. The fastest speedup recorded is 14x with 32 nodes. Note that the ring reduce has a higher speedup than the crossbar because when the interconnect gets more congested, the crossbar architecture is more efficient than the ring architecture. This is because the crossbar architecture does not involve any message forwarding.</p> <p>When using the tree interconnect, the tree reduce is around the same speed as the naive reduce when the number of nodes is small. However, as the number of nodes increases, the tree reduce sees the largest speedup. The fastest speedup is 50x with 32 nodes. While the optimized version perfectly fits the tree architecture's structure, the naive version has to send the message to every other node. Because the tree has a root, a single point of contention, the unoptimized version doesn't know about this contention and results in extreme inefficiency. Due to this contention, the tree architecture is the slowest when using the naive reduce.</p>"},{"location":"final/#local-barrier","title":"Local Barrier","text":"<p>In this test, we see that the optimized barrier operation is not significantly different in speed on the crossbar architecture. This is expected since the SSMR optimization doesn't apply if all messages are direct. In the ring and tree architectures, the optimized barrier operation is slightly faster because the \"return echo\" of the barrier operation can benefit from the SSMR optimization.</p>"},{"location":"final/#local-bcast","title":"Local Bcast","text":"<p>When using the crossbar interconnect, the optimized version is ever so slightly slower than the non-optimized version, indicating that SSMR does also have overhead. However, when the number of nodes increases, the optimized version becomes faster, since they don't wait for the message to propagate fully. Instead, the node can choose to start the next operation early. The most that this strategy can save is the \"return echo\" of the bcast operation, which should save around 2x. This is exactly what we observe.</p> <p>When using the ring or tree interconnects, the SSMR and the latency hiding strategy are particularly effective. The fastest speedup is 10x with 32 nodes for ring and 4.5x with 32 nodes for tree. </p>"},{"location":"final/#global-tests","title":"Global Tests","text":"<p>In the following tests, we run the MPI operations on a two browsers with evenly split number of workers. The two browsers are running on the same machine for this test. It demonstrates the same principles as using multiple machines, but with less noise. Running on multiple machines should expect much more slowdown due network latency.</p> <p>The speedup is calculated by comparing the time taken for the unoptimized version and the optimized version. The optimized version has the SSMR and the global optimizations, while the unoptimized version does not.  Note: the raw time between different operation are not comparable, since they are run for a different number of iterations. </p>"},{"location":"final/#global-broadcast","title":"Global Broadcast","text":"<p>The unoptimized global broadcast time is slightly surprising. When there's less than 4 nodes, the operation is very fast. However, as soon as it hits 8 nodes, the operation becomes very slow. The reason for this is because there is some sort of buffer in the low level WebSocket interface. We performed regular send and receive tests, and found that if first 2 back-to-back messages are sent immediately, but any subsequent messages are queued and sent in a group after around 50ms. We reason that this is a optimization that is made by the WebSocket, and it is out of our control. Because of this behavior, we observe the cliff in the graph.</p> <p>The optimized global broadcast time is significantly faster than the unoptimized version. First, we avoid the cliff in the graph by condensing all the messages into one message. Second, it uses SSMR avoid the overhead of sending the same message multiple times. The optimized version time graph makes sense, since it only sends one message at a time. The time it takes gradually increases as the number of nodes increases, as expected.  As a result, we get some speedup at lower processor counts, but the largest speedup is at higher processor counts. However, the speedup decreases at higher processor counts, since the unoptimized speed is largely constant. The fastest speedup is 100x with 64 nodes.</p>"},{"location":"final/#global-barrier","title":"Global Barrier","text":"<p>Again, we observe the cliff in the unoptimized global barrier time at 8 nodes. This is likely due to the same reason as described above. The optimized global barrier avoids this cliff by first performing a barrier locally, then performing a secondary barrier globally. This way, the number of messages sent over the WebSocket server is reduced significantly. True, there are now two stages of barriers, but this is still faster since it reduces the bottleneck. The optimized version time graph makes sense, since it gradually increases as the number of nodes increases.  As a result, we get some speedup at lower processor counts, but the largest speedup is at higher processor counts. However, the speedup decreases at higher processor counts, since the unoptimized speed is largely constant. The fastest speedup is 230x with 64 nodes.</p>"},{"location":"final/#global-reduce","title":"Global Reduce","text":"<p>The unoptimized global reduce time is slow, with an upward trend as the number of nodes increases. This is because the operation is naive and sends more data as there are more nodes. Interestingly, the optimized version uses around the same time up till 16 nodes, then starts to significantly slow down. We suspect that this is because of the machine that we are using only having 20 cores. Thus, the OS is forced to context switch, and since reduce is a computation heavy operation (unlike barrier and bcast), this slows down the operation. As a result, the speedup is fastest when there are 16 nodes, at a little over 50x.</p>"},{"location":"final/#conclusion","title":"Conclusion","text":"<p>This project is much larger and more complex than we initially anticipated. We learned a lot not only about MPI, but also about the browser environment, identifying relevant bottlenecks, and optimizing for interconnects. In the end, this project has over 3000 lines of code, including the library and documentation.  Although we could not implement the non-blocking APIs (due to the single-threaded nature of JS), we spent the time to optimize the collective APIs instead. We are proud of the results we achieved. </p> <p>As planned in the proposal, we ran a MPI program on our system (the sqrt program provided in asst4), and found our system to only perform ~10x slower than the <code>-O3</code> optimized C version. This is a significant achievement, considering the overhead of running a scripted language in the browser. We also compared using local channels and global WebSocket channels, and found that the global channels are significantly slower (and thus, we optimized accordingly).</p> <p>This project is truly one-of-a-kind, there is really not much out there that does what it does. We hope our work not only served as a good learning experience for us, but also could be inspiring to others in the community.</p> <p>Here's the documentation link again for this project:</p> <p>https://the-parallel-gods.github.io/mpi.js/docs/</p>"},{"location":"final/#contribution","title":"Contribution","text":""},{"location":"final/#sean-haoxians-50","title":"Sean (haoxians) - 50%","text":"<ul> <li> MPI front end</li> </ul> <p>Sean</p>"},{"location":"final/#david-drudo-50","title":"David (drudo) - 50%","text":"<ul> <li> MPI backend end</li> </ul> <p>David</p>"},{"location":"final_sean/","title":"Final sean","text":""},{"location":"final_sean/#project-mpijs","title":"PROJECT: MPI.JS","text":""},{"location":"final_sean/#url","title":"URL","text":"<p>https://the-parallel-gods.github.io/mpi.js/home/</p>"},{"location":"final_sean/#summary","title":"SUMMARY","text":"<p>We want to create an MPI library in the browser using JavaScript, implement some of the APIs (bcast, barrier, all_reduce, ...), use them to run MPI programs, and optimize the MPI collective APIs given the browser environment.</p>"},{"location":"final_sean/#background","title":"BACKGROUND","text":"<p>MPI has never been implemented in JavaScript in the browser before, so we are bound to run into many new interesting problems specific to the runtime environment. In this section, we provide some background information on how JavaScript works and describe the challenges it poses. In the next section, we will describe our approach to solving these challenges.</p>"},{"location":"final_sean/#processes-inside-a-browser-are-very-isolated","title":"Processes inside a browser are very isolated","text":"<p>For good security reasons, each process in the browser is very isolated from the others. This means that there is no shared memory between processes, and communication between processes is limited. The only types of communication that are allowed are point-to-point <code>MessageChannel</code>s and <code>BroadcastChannel</code>s. To make things worse, <code>BroadcastChannel</code>s are rate-limited one message per 100ms, which is too slow. Effectively, we are forced to use <code>MessageChannel</code>s for our communication inside a browser.</p>"},{"location":"final_sean/#javascript-is-event-driven-single-threaded-language","title":"JavaScript is event-driven single-threaded language","text":"<p>Normally, JavaScript is designed for UI. What event-driven means in that context is every flow of control is initiated by an event, like a button press. Moreover, since it is single-threaded, no other code gets a chance to run until the function for that event finishes. This is a problem for parallel programming, because we can't just spawn a new thread to do some work in the background. This is a huge limitation when it comes to supporting non-blocking MPI operations.</p>"},{"location":"final_sean/#the-browser-cannot-create-a-websocket-server","title":"The browser cannot create a WebSocket server","text":"<p>In this project, we want to support MPI programs that run across multiple computers over the Internet. Since the browser cannot create raw TCP or UDP sockets, the best option is to use WebSockets, which is a higher-level protocol built on top of TCP. However, since the browser does not have the permission to create servers, we need to have a centralized WebSocket server that all the browsers connect to. This server will be responsible for routing messages between the browsers.</p>"},{"location":"final_sean/#browser-comes-with-a-ui","title":"Browser comes with a UI","text":"<p>Since the browser already provides an HTML UI, we can use it to show the status of the MPI program. In this project, we will take advantage of this by creating a live dashboard that shows the diagnostic information of the MPI program in real-time.</p>"},{"location":"final_sean/#approach","title":"APPROACH","text":"<p>Our system architecture is inspired by network gateways at the global level and hardware architectures at the local level.  We design our system to have a centralized WebSocket server that all the browsers connect to as well as a static file server. Inside each browser tab, the system creates many worker processes that run the user MPI code, which are all connected by hot-swappable interconnect architectures.</p>"},{"location":"final_sean/#address-naming","title":"Address naming","text":"<p>Since this project involves significant routing work, here we formally clarify the address naming scheme we use. </p> <ul> <li>GR_ID: Global Router ID (unique identifier for each browser)</li> <li>NR_ID: Node Router ID (local unique identifier for each worker process, starts from 0 for each browser)</li> <li>PID: Node Router ID (global unique identifier for each worker process, continuous across browsers)</li> <li>NR_OFFSET: Node Router Offset (smallest PID in the local network)</li> </ul> <p>The system is designed this way so that the user can use an abstraction that gives the illusion of every worker process being in the same global network; however, the system under the hood is designed to be as optimized as possible.</p> <p>Example system:</p> <pre><code>Browser 1: GR_ID=0\n    Worker 1: PID=0, NR_ID=0, NR_OFFSET=0\n    Worker 2: PID=1, NR_ID=1, NR_OFFSET=0\n    Worker 3: PID=2, NR_ID=2, NR_OFFSET=0\n\nBrowser 2: GR_ID=1\n    Worker 1: PID=3, NR_ID=0, NR_OFFSET=3\n    Worker 2: PID=4, NR_ID=1, NR_OFFSET=3\n    Worker 3: PID=5, NR_ID=2, NR_OFFSET=3\n</code></pre>"},{"location":"final_sean/#websocket-server","title":"WebSocket Server","text":"<p>The WebSocket server is responsible for routing messages between the browser. It assigns each browser a unique GR_ID and keeps track of the global routing table. Since this central server is a point of contention, we designed it to be as lightweight as possible, and we offload as much work as possible to the browser. Each request to the server is a simple JSON object that contains the message and the destination GR_ID. The websocket server uses SSMR optimization (described later).</p>"},{"location":"final_sean/#static-file-server","title":"Static File Server","text":"<p>The static file server is responsible for serving the user's MPI code, the MPI.js library, and the UI files. It is a simple HTTP server that supports hot loading the user's code into the browser.</p>"},{"location":"final_sean/#global-router","title":"Global Router","text":"<p>Sitting in the Browser UI process, the Global Router is responsible for routing messages between browsers. Whenever a Node Router wants to send a message to another browser, it delegates the message to the Global Router. The Global Router then forwards the message to the destination browser's Global Router, which then forwards the message to the destination Node Router. When messages get to the Global Router level, the PID and the NR_IDs are abstracted away, and the system only deals with GR_IDs. This is done to make the system more scalable and to hide the complexity of the system at each layer. The Global Router uses SSMR optimization (described later), so if it needs to send the same message to multiple other Global Routers, it only needs to send one message to the WebSocket server.</p>"},{"location":"final_sean/#node-router","title":"Node Router","text":"<p>In the worker process, there is a Node Router is responsible for routing, queueing, and feeding message to and from the user's MPI code. The Node Router is responsible for routing messages between workers within the same browser. The Node Router uses a custom routing table to determine the best route to send a message to another worker process. The Node Router uses SSMR optimization (described later) to reduce the number of messages sent.</p> <p>In the best case, the interconnect that connects the Node Routers within the same browser is a crossbar, which allows any message to be sent to any other worker process with one hop. However, when more workers are needed, the number of connections grows quadratically, so we also support ring and tree interconnects that balance the number of connections and the number of hops.</p>"},{"location":"final_sean/#doubly-indexed-database","title":"Doubly Indexed Database","text":"<p>Whenever a Node Router receives a message, it needs to feed that message to the user's MPI code. In our JavaScript MPI implementation, we skip the back and forth checking that actual MPI implementations do, in order to improve performance. Instead, we directly deposit the message into a queue. Since the system is in a browser, where memory usage is already very high without MPI, we delegate the responsibility of not overflowing the queue to the MPI user. Since JavaScript is single-threaded and thread-safe, we can construct a very performant ProducerConsumer queue without locks. </p> <pre><code># Pseudo code\nmsg_queue = DB()\nrecv_queue = DB()\n\ndef on_message(msg, tag, src_pid):\n    if recv_queue.has(tag, src_pid):\n        recv_queue.call(msg)\n    else:\n        msg_queue.add(msg, tag, src_pid)\n\ndef user_request_recv(tag, src_pid, callback):\n    if msg_queue.has(tag, src_pid):\n        callback(msg_queue.call(tag, src_pid)) # continue on user code with received message\n    else:\n        recv_queue.add(tag, src_pid, callback) # when the message arrives, it'll call my callback\n\n</code></pre> <p>Our focus then shifts to making the queue as efficient as possible, since many messages can be waiting there. This would have been simple, if not for the tags and src_pids of the messages. MPI supports having users receive messages with only specific tags and from specific processes. If the number of messages in the queue is very high, the search through all of them to find the right message will be very slow. </p> <p>between worker processes in the same browser, much like a hardware interconnect. The best case is when the </p> <p>One of the key structures we implemented to further our analogy of individual tabs acting as cores of a cpu were interconnects. Designed as a custom routing table, mpi.js supports a ring, tree, and crossbar interconnect for analysis. Our interconnects define the potential routes a node can use when sending messages across the local or global network of processes. The primary challenge when applying these interconnects to a highly distributed, parallel environment was setting a scheme where \"node ids\" required as little translation as possible to reduce any potential unnecessary overhead during sends and receives. Our \"trick\" to solve this is to have two sets of routers that help enable fast and efficient communication between nodes without burdening each tab to calculate the optimal route necessary for broadcast, reduce, or any other function. Our \"Node Router\" focuses on handling messages within a local network. This is how messages between tabs on the same browser are handled. If a message needs to be sent to another a machine and go across our global network we designed a \"Global Router.\" The Global Router is connected to a GR network that on initialization does a handshake so that the centralized GR server knows all the routing information necessary. This allows the Global Routers to focus most of their computation on choosing how to forward incoming messages to the Node Router and leave most of the complex routing information to the central server. The \"GR Server\" has a modifiable routing table to ensure flexibility and fast forwarding. This server is also responsible for the designation of global identifiers that are essential in operations such as reduce and gather/scatter. This structure of routers allows for a divide-and-conquer break down of routing operations so that no one process is overburdened and our MPI worker tabs can remain primarily focused on strict computation unless specifically awaiting for a particular message.</p> <p></p>"},{"location":"final_sean/#results","title":"RESULTS","text":""},{"location":"final_sean/#local-tests","title":"Local Tests","text":"<p>Feel free to write some explanation for each graph, if some speedup/time don't make sense (the global ones), skip them, I'll do them later. I'll expand on them later if needed</p> <p>You got it 8======D---&lt;   xD</p>"},{"location":"final_sean/#allreduce","title":"Allreduce","text":"<p>ring speedup, tree speedup, crossbar speedup</p> <p></p> <p></p> <p></p>"},{"location":"final_sean/#barrier","title":"Barrier","text":"<p>ring speedup, tree speedup, crossbar speedup</p> <p></p> <p></p> <p></p>"},{"location":"final_sean/#bcast","title":"Bcast","text":"<p>ring speedup, tree speedup, crossbar speedup</p> <p></p> <p></p> <p></p>"},{"location":"final_sean/#global-tests","title":"Global Tests","text":""},{"location":"final_sean/#broadcast","title":"Broadcast","text":"<p>#### Reduce</p>"},{"location":"final_sean/#barrier_1","title":"Barrier","text":"<p>At this moment, we are at 14 pages. oU/images/benchmarks/Time_(ms)_Global_Optimized_Broadcast_Time.png) </p> <p>BarrierUno### Sean (haoxians) - 50%</p> <ul> <li> MPI front end</li> </ul> <p>Sean</p>"},{"location":"final_sean/#david-drudo-50","title":"David (drudo) - 50%","text":"<ul> <li> MPI backend end</li> </ul> <p>David</p>"},{"location":"home/","title":"MPI.js","text":"<p>MPI, now on the web.</p>"},{"location":"home/#the-vision","title":"The vision","text":"<p>Our mission is to revolutionize impractical distributed computing by providing a browser-based MPI implementation that empowers researchers and developers to seamlessly explore parallel programming concepts and deploy distributed applications across diverse platforms.</p>"},{"location":"home/#for-developers","title":"For developers","text":"<p>Get Started</p> <p>Github</p>"},{"location":"home/#roadmap","title":"Roadmap","text":"<ul> <li> Make a nice documentation website</li> <li> Implement the following APIs</li> <li> Basics<ul> <li> MPI_Init</li> <li> MPI_Finalize</li> <li> MPI_Abort</li> <li> MPI_Comm_size</li> <li> MPI_Comm_rank</li> </ul> </li> <li> P2P<ul> <li> MPI_Send</li> <li> MPI_Isend</li> <li> MPI_Recv</li> <li> MPI_Irecv</li> </ul> </li> <li> Barriers &amp; Broadcasts<ul> <li> MPI_Barrier</li> <li> MPI_Bcast</li> <li> MPI_Ibcast</li> </ul> </li> <li> Gathers<ul> <li> MPI_Gather</li> <li> MPI_Gatherv</li> <li> MPI_Allgather</li> <li> MPI_Allgatherv</li> </ul> </li> <li> Scatters<ul> <li> MPI_Scatter</li> <li> MPI_Scatterv</li> </ul> </li> <li> Reduces<ul> <li> MPI_Reduce</li> <li> MPI_Allreduce</li> </ul> </li> <li> Write the tool that transplants user JS code into runnable MPI.js code</li> <li> Compare different communication methods (channels/WebSockets) between local threads</li> <li> Use WebSockets to connect MPI nodes across multiple computers</li> <li> Optimize for memory usage by using clever message-routing techniques</li> <li> Run benchmarks on all variations of our API</li> <li> Make a live dashboard that shows how many msgs/sec are happening in real-time</li> </ul>"},{"location":"home/#about-us","title":"About us","text":"<p>We are the-parallel-gods</p> <p>Sean and David</p>"},{"location":"home/#support-warranty","title":"Support &amp; warranty","text":"<p>lmao</p>"},{"location":"milestone/","title":"Milestone","text":""},{"location":"milestone/#project-mpijs-milestone-report","title":"PROJECT: MPI.JS -- Milestone Report","text":""},{"location":"milestone/#url","title":"URL","text":"<p>https://the-parallel-gods.github.io/mpi.js/home/</p>"},{"location":"milestone/#background-platform-choice","title":"BACKGROUND &amp; PLATFORM CHOICE","text":"<p>Atwood\u2019s Law: \u201cAny application that can be written in JavaScript, will eventually be written in JavaScript.\u201d</p> <p>JavaScript is one of the most popular languages used by many developers. It also runs across diverse platforms. To make MPI accessible to more programmers and compatible with more devices, we introduce it to JavaScript.</p> <p>As described in #the-challenge, JS is an inherently single-threaded programming language. Thus, shared-memory parallel models similar to OpenMP are simply not possible. However, we can still pass messages between multiple \"isolated threads\" in JS. In such an environment, MPI becomes the perfect solution for parallel programming.</p>"},{"location":"milestone/#summary-on-progress","title":"Summary On Progress","text":"<p>We made huge progress on the project. </p> <p>The first big challenge was to make a transpiler that can convert user code into runnable MPI.js code. This involved writing a full stack server, with front-end code that can create the web workers and link with the UI, the backend code to dynamically route the user MPI code, and hot load it into the web workers, and finally the web workers themselves that receives all the system configurations and channel information. The scope of this was way beyond what we initially dreamed of, but after long hours, it's a miracle that it works.</p> <p>We then created the <code>node_router</code> which magically queues up messages by other nodes and feeds them to callbacks, supporting message tags. We optimized this data structure for 4 iterations, and finally optimized it to be O(1) time for all operations. After this, we implemented some basic send and receive MPI functions, and tested them with the transpiler. With much work-around (as described below), we also hacked together a way to support async sends and receives in a single-threaded language.</p> <p>After that, we developed a low-overhead, automatic diagnostic collection system that can be used to monitor the performance of various functions in the MPI.js system. This data is then live-streamed to a real-time dashboard that shows the number of messages being sent and received per second, as well as the time spent in each function vs the time spent on actual computation. Along with this, we developed a simple to use SmartDashboard that allows users to draw graphs, charts, progress bars, and logs in real-time, to help them debug their code.</p> <p>Then, we implemented the <code>sqrt</code> MPI program (from the asst4 example) and found that our JavaScript system is able to perform only 10x slower than pure <code>-O3</code> heavily optimized C code, which is a huge win. Our benchmark also shows that our system can handle up to 100,000+ message sends and receives per second.</p> <p>Finally, we made a very detailed documentation for every single class, every single data type, and every single function in our system. This will greatly help users to program using our library.</p> <p>Here's a screenshot of our live-dashboard:</p> <p></p>"},{"location":"milestone/#current-project-status","title":"Current Project Status","text":"<ul> <li> Make a nice documentation website</li> <li> Implement the following APIs</li> <li> Basics<ul> <li> MPI_Init</li> <li> MPI_Finalize</li> <li> MPI_Abort</li> <li> MPI_Comm_size</li> <li> MPI_Comm_rank</li> </ul> </li> <li> P2P<ul> <li> MPI_Send</li> <li> MPI_Isend</li> <li> MPI_Recv</li> <li> MPI_Irecv</li> </ul> </li> <li> Barriers &amp; Broadcasts<ul> <li> MPI_Barrier</li> <li> MPI_Ibarrier</li> <li> MPI_Bcast</li> <li> MPI_Ibcast</li> </ul> </li> <li> Gathers<ul> <li> MPI_Gather</li> <li> MPI_Gatherv</li> <li> MPI_Igather</li> <li> MPI_Igatherv</li> <li> MPI_Allgather</li> <li> MPI_Allgatherv</li> <li> MPI_Iallgather</li> </ul> </li> <li> Scatters<ul> <li> MPI_Scatter</li> <li> MPI_Scatterv</li> <li> MPI_Iscatter</li> <li> MPI_Iscatterv</li> </ul> </li> <li> Reduces<ul> <li> MPI_Reduce</li> <li> MPI_Ireduce</li> <li> MPI_Allreduce</li> <li> MPI_Iallreduce</li> </ul> </li> <li> Write the tool that transplants user JS code into runnable MPI.js code</li> <li> Compare different communication methods (channels/WebSockets) between local threads</li> <li> Use WebSockets to connect MPI nodes across multiple computers</li> <li> Optimize for memory usage by using clever message-routing techniques</li> <li> Run benchmarks on all variations of our API</li> </ul>"},{"location":"milestone/#hope-to-achieve","title":"HOPE TO ACHIEVE","text":"<ul> <li> Make a live dashboard that shows how many msgs/sec are happening in real-time</li> <li> Make a debugging dashboard that replays the communications during computation~~</li> <li> Implement the following APIs</li> <li> MPI_Scan</li> <li> MPI_Exscan</li> </ul>"},{"location":"milestone/#goals-being-removed","title":"Goals Being Removed","text":"<p>After considering our progress and discoveries made while working on mpi.js, we've decided to no longer pursue the idea of a \"debugging\" dashboard. The amount of memory necessary to hold the timeline information necessary to implement such a dashboard is too large to be feasibly completed without detracting from the original project. Since we still need to work on the different routing strategies for reduce, gather, and scatter it is also more likely that we will not have time to implement scan since we want to dedicate time to have interesting data to compare. </p> <p>We also dropped \"fast\" support for async MPI functions (we will keep the slow version). This is because JavaScript is a single-threaded event-driven language, meaning any function when started cannot be interrupted. Thus, when the user code is running, messages cannot be received. While we found a way around it (by explicitly descheduling the user code, and delegating the server to re-trigger a new event for the remainder of the user code to be resumed after any messages are checked for), it is incredibly complex and inefficient. We decided that the speed of this cannot be improved due to the nature of JavaScript. Thus, while all async function calls will behave correctly and be supported, they are highly discouraged.</p>"},{"location":"milestone/#issues-that-concern-us","title":"Issues that Concern Us","text":"<p>Not much. In the proposal feedback, we are recommended to perform more performance analysis and comparison. We plan to do this in architectural optimizations. Since in our system, local communications are fast and global communications are slow between machines, we wish to implement and benchmark several versions of the reduce function. Specifically, we want to use ring reduce locally first, and then tree reduce between the machines. </p> <p>We also wish to explore the possibility of workers not being fully connected by channels within a machine, and the routing optimizations that we can perform in that system hierarchy. Finally, we will run the baseline of having websockets connect every worker (in the worst case scenario) and compare it to the optimized versions.</p> <p>Suggestions on any other ideas for performance optimizations and comparisons that can be done is welcome and appreciated.</p>"},{"location":"milestone/#intentions-for-poster-session","title":"Intentions For Poster Session","text":"<p>Our intention is to use our in-development dashboard as our main presentation tool. Since it shows a break down of time a process spends sending, receiving, waiting, and actually computing it's a great way to visually show the relative speed up we are providing to a program. We also plan to show some pre-captured measurements from the dashboard so that the demo doesn't have to manually run through every variation of our mpi configurations we want to analyze.</p>"},{"location":"milestone/#updated-schedule","title":"Updated SCHEDULE","text":"Date Sean David 3/28 Submit Proposal Submit Proposal 4/10 Set up P2P Set up Basics API 4/11 Finish JS transplanter Set up route forwarding data 4/13 Live Dashboard Set up Websocket support with current API 4/17 (By Checkpoint) Set up routing local protocol options Set up global protocol options 4/24 Gather, Scatter Reduce/Broadcast 5/3 Set up local results downloader 5/4 Submit Report Submit Report 5/6 Demo Day Demo Day"},{"location":"milestone/#about-us","title":"ABOUT US","text":""},{"location":"milestone/#sean-haoxians","title":"Sean (haoxians)","text":"<p>Sean</p>"},{"location":"milestone/#david-drudo","title":"David (drudo)","text":"<p>David</p>"},{"location":"proposal/","title":"Proposal","text":""},{"location":"proposal/#project-mpijs","title":"PROJECT: MPI.JS","text":""},{"location":"proposal/#url","title":"URL","text":"<p>https://the-parallel-gods.github.io/mpi.js/home/</p>"},{"location":"proposal/#summary","title":"SUMMARY","text":"<p>We want to create an MPI library in the browser using JavaScript, implement some of the APIs (bcast, all_gather, all_reduce, ...), optimize them given the browser environment, and benchmark them using existing MPI programs.</p>"},{"location":"proposal/#background-platform-choice","title":"BACKGROUND &amp; PLATFORM CHOICE","text":"<p>Atwood\u2019s Law: \u201cAny application that can be written in JavaScript, will eventually be written in JavaScript.\u201d</p> <p>JavaScript is one of the most popular languages used by many developers. It also runs across diverse platforms. To make MPI accessible to more programmers and compatible with more devices, we introduce it to JavaScript.</p> <p>As described in #the-challenge, JS is an inherently single-threaded programming language. Thus, shared-memory parallel models similar to OpenMP are simply not possible. However, we can still pass messages between multiple \"isolated threads\" in JS. In such an environment, MPI becomes the perfect solution for parallel programming.</p>"},{"location":"proposal/#the-challenge","title":"THE CHALLENGE","text":"<ul> <li>MPI has never been implemented in JavaScript in the browser before, so we may run into many new problems specific to the runtime environment</li> <li>JavaScript is a single-threaded language, so we need to find alterative ways to perform parallel work</li> <li>Each thread in the browser has much isolation for security reasons, (ie no shared memory), so communication methods are limited</li> <li>We will have to carefully study how the APIs are implemented in MPI, and optimize them for the JavaScript environment's characteristics</li> <li>We need to carefully design and optimize the communication hierarchy of MPI nodes and MPI threads, since we wish to support nodes across multiple computers over the Internet</li> <li>We need to convert existing MPI programs into our JavaScript framework to benchmark our speedup performance</li> </ul>"},{"location":"proposal/#resources","title":"RESOURCES","text":"<ul> <li>We will develop the library from scratch on our laptops, to begin with</li> <li>When finished, we will attempt to install it onto PSC to test the speedup</li> <li>We will reference OpenMPI to learn how to implement the APIs (https://github.com/open-mpi)</li> </ul>"},{"location":"proposal/#plan-to-achieve","title":"PLAN TO ACHIEVE","text":"<ul> <li> Make a nice documentation website</li> <li> Implement the following APIs</li> <li> Basics<ul> <li> MPI_Init</li> <li> MPI_Finalize</li> <li> MPI_Abort</li> <li> MPI_Comm_size</li> <li> MPI_Comm_rank</li> </ul> </li> <li> P2P<ul> <li> MPI_Send</li> <li> MPI_Isend</li> <li> MPI_Recv</li> <li> MPI_Irecv</li> </ul> </li> <li> Barriers &amp; Broadcasts<ul> <li> MPI_Barrier</li> <li> MPI_Ibarrier</li> <li> MPI_Bcast</li> <li> MPI_Ibcast</li> </ul> </li> <li> Gathers<ul> <li> MPI_Gather</li> <li> MPI_Gatherv</li> <li> MPI_Igather</li> <li> MPI_Igatherv</li> <li> MPI_Allgather</li> <li> MPI_Allgatherv</li> <li> MPI_Iallgather</li> </ul> </li> <li> Scatters<ul> <li> MPI_Scatter</li> <li> MPI_Scatterv</li> <li> MPI_Iscatter</li> <li> MPI_Iscatterv</li> </ul> </li> <li> Reduces<ul> <li> MPI_Reduce</li> <li> MPI_Ireduce</li> <li> MPI_Allreduce</li> <li> MPI_Iallreduce</li> </ul> </li> <li> Write the tool that transplants user JS code into runnable MPI.js code</li> <li> Compare different communication methods (channels/WebSockets) between local threads</li> <li> Use WebSockets to connect MPI nodes across multiple computers</li> <li> Optimize for memory usage by using clever message-routing techniques</li> <li> Run benchmarks on all variations of our API</li> </ul>"},{"location":"proposal/#hope-to-achieve","title":"HOPE TO ACHIEVE","text":"<ul> <li> Make a live dashboard that shows how many msgs/sec are happening in real-time</li> <li> Make a debugging dashboard that replays the communications during computation</li> <li> Implement the following APIs</li> <li> MPI_Scan</li> <li> MPI_Exscan</li> </ul>"},{"location":"proposal/#deverliables","title":"DEVERLIABLES","text":""},{"location":"proposal/#the-final-framework-should","title":"The final framework should...","text":"<ul> <li>...achieve reasonable speedup (not absolute speed, due to JS) for MPI programs that are not extremely communication-heavy on a single-computer setup</li> <li>...successfully run on a smartphone with a compatible browser</li> <li>...successfully run across multiple computers over the Internet</li> <li>...not be run on anyone's smart toaster : )</li> </ul>"},{"location":"proposal/#demo-day","title":"DEMO DAY","text":"<ul> <li>We will show the speedup graph of running existing MPI programs on the poster</li> <li>We will set up a laptop to produce a live demo of running MPI in the browser</li> </ul>"},{"location":"proposal/#schedule","title":"SCHEDULE","text":"Date Sean David 3/28 Submit Proposal Submit Proposal 4/10 Set up P2P Set up Basics API 4/11 Finish JS transplanter 4/13 Set up local results downloader Gather, Scatter 4/17 (By Checkpoint) Reduce/Broadcast 4/24 Websocket support with current API Set up routing protocol options 5/3 Live Dashboard Debugging Data Collection 5/4 Submit Report Submit Report 5/6 Demo Day Demo Day"},{"location":"proposal/#about-us","title":"ABOUT US","text":""},{"location":"proposal/#sean-haoxians","title":"Sean (haoxians)","text":"<p>Sean</p>"},{"location":"proposal/#david-drudo","title":"David (drudo)","text":"<p>David</p>"}]}